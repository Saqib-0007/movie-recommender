{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f710be",
   "metadata": {},
   "source": [
    "# Movie Rating Prediction - Machine Learning Project\n",
    "\n",
    "This project trains a machine learning model to predict movie ratings and evaluates it using MAE, MSE, RMSE, and R² metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "003a3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell if needed)\n",
    "# !pip install pandas numpy scikit-learn matplotlib seaborn scipy xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "852f81a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from scipy.stats import randint, uniform\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "import json\n",
    "import ast\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418aba37",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d834809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ratings dataset (memory-optimized)...\n",
      "   - Reading dataset in chunks and sampling up to 3,000,000 rows...\n",
      "     - Loaded 1,000,000 rows...\n",
      "     - Loaded 2,000,000 rows...\n",
      "     - Loaded 3,000,000 rows...\n",
      "\n",
      "✅ Dataset loaded successfully!\n",
      "Ratings Data Shape: (3000000, 4)\n",
      "Memory usage: 57.22 MB\n",
      "\n",
      "First few rows:\n",
      "   userId  movieId  rating   timestamp\n",
      "0       1      110     1.0  1425941529\n",
      "1       1      147     4.5  1425942435\n",
      "2       1      858     5.0  1425941523\n",
      "3       1     1221     5.0  1425941546\n",
      "4       1     1246     5.0  1425941556\n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000000 entries, 0 to 2999999\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   userId     int32  \n",
      " 1   movieId    int32  \n",
      " 2   rating     float32\n",
      " 3   timestamp  int64  \n",
      "dtypes: float32(1), int32(2), int64(1)\n",
      "memory usage: 57.2 MB\n",
      "None\n",
      "\n",
      "Basic Statistics:\n",
      "             userId       movieId        rating     timestamp\n",
      "count  3.000000e+06  3.000000e+06  3.000000e+06  3.000000e+06\n",
      "mean   1.553421e+04  1.573393e+04  3.533633e+00  1.169960e+09\n",
      "std    9.025891e+03  3.090353e+04  1.053801e+00  2.054970e+08\n",
      "min    1.000000e+00  1.000000e+00  5.000000e-01  8.250017e+08\n",
      "25%    7.609000e+03  1.061000e+03  3.000000e+00  9.868580e+08\n",
      "50%    1.542400e+04  2.572000e+03  3.500000e+00  1.149450e+09\n",
      "75%    2.334100e+04  6.385000e+03  4.000000e+00  1.356313e+09\n",
      "max    3.133200e+04  1.762710e+05  5.000000e+00  1.501822e+09\n"
     ]
    }
   ],
   "source": [
    "# Load the ratings dataset with memory optimization\n",
    "print(\"Loading ratings dataset (memory-optimized)...\")\n",
    "\n",
    "# Option 1: Read in chunks and sample if needed\n",
    "USE_FULL_DATA = False  # Set to True to use all 26M rows (requires 8GB+ RAM)\n",
    "CHUNK_SIZE = 500000  # Read 500K rows at a time\n",
    "MAX_ROWS = 3000000 if not USE_FULL_DATA else None  # Limit to 3M rows if sampling\n",
    "\n",
    "if USE_FULL_DATA:\n",
    "    # Read full dataset with optimized dtypes\n",
    "    print(\"   - Reading full dataset with optimized data types...\")\n",
    "    ratings_df = pd.read_csv(\n",
    "        'data set/ratings.csv',\n",
    "        dtype={\n",
    "            'userId': 'int32',\n",
    "            'movieId': 'int32',\n",
    "            'rating': 'float32',\n",
    "            'timestamp': 'int64'\n",
    "        },\n",
    "        low_memory=False\n",
    "    )\n",
    "else:\n",
    "    # Read in chunks and sample for memory efficiency\n",
    "    print(f\"   - Reading dataset in chunks and sampling up to {MAX_ROWS:,} rows...\")\n",
    "    chunks = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(\n",
    "        'data set/ratings.csv',\n",
    "        chunksize=CHUNK_SIZE,\n",
    "        dtype={\n",
    "            'userId': 'int32',\n",
    "            'movieId': 'int32',\n",
    "            'rating': 'float32',\n",
    "            'timestamp': 'int64'\n",
    "        },\n",
    "        low_memory=False\n",
    "    ):\n",
    "        if MAX_ROWS and total_rows >= MAX_ROWS:\n",
    "            break\n",
    "        \n",
    "        remaining = MAX_ROWS - total_rows if MAX_ROWS else len(chunk)\n",
    "        if MAX_ROWS and len(chunk) > remaining:\n",
    "            chunk = chunk.head(remaining)\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "        if total_rows % 1000000 == 0:\n",
    "            print(f\"     - Loaded {total_rows:,} rows...\")\n",
    "        \n",
    "        if MAX_ROWS and total_rows >= MAX_ROWS:\n",
    "            break\n",
    "    \n",
    "    ratings_df = pd.concat(chunks, ignore_index=True)\n",
    "    del chunks  # Free memory\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n✅ Dataset loaded successfully!\")\n",
    "print(f\"Ratings Data Shape: {ratings_df.shape}\")\n",
    "print(f\"Memory usage: {ratings_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(ratings_df.head())\n",
    "print(\"\\nData Info:\")\n",
    "print(ratings_df.info(memory_usage='deep'))\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(ratings_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51dfab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies Data Shape: (45466, 24)\n",
      "\n",
      "Movies Columns: ['adult', 'belongs_to_collection', 'budget', 'genres', 'homepage', 'id', 'imdb_id', 'original_language', 'original_title', 'overview']\n"
     ]
    }
   ],
   "source": [
    "# Load movies metadata for additional features\n",
    "movies_df = pd.read_csv('data set/movies_metadata.csv', low_memory=False)\n",
    "print(\"Movies Data Shape:\", movies_df.shape)\n",
    "print(\"\\nMovies Columns:\", movies_df.columns.tolist()[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b851d886",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cc89cd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating user statistics...\n",
      "Calculating movie statistics...\n",
      "User Statistics Shape: (31332, 4)\n",
      "Movie Statistics Shape: (26511, 4)\n",
      "\n",
      "Data types check:\n",
      "ratings_df userId dtype: int32\n",
      "user_stats userId dtype: int32\n",
      "ratings_df movieId dtype: int32\n",
      "movie_stats movieId dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# Calculate user and movie statistics for feature engineering\n",
    "print(\"Calculating user statistics...\")\n",
    "user_stats = ratings_df.groupby('userId')['rating'].agg(['mean', 'count', 'std']).reset_index()\n",
    "user_stats.columns = ['userId', 'user_avg_rating', 'user_rating_count', 'user_rating_std']\n",
    "user_stats = user_stats.fillna(0)\n",
    "\n",
    "print(\"Calculating movie statistics...\")\n",
    "movie_stats = ratings_df.groupby('movieId')['rating'].agg(['mean', 'count', 'std']).reset_index()\n",
    "movie_stats.columns = ['movieId', 'movie_avg_rating', 'movie_rating_count', 'movie_rating_std']\n",
    "movie_stats = movie_stats.fillna(0)\n",
    "\n",
    "# Ensure merge keys have same data type\n",
    "user_stats['userId'] = user_stats['userId'].astype(ratings_df['userId'].dtype)\n",
    "movie_stats['movieId'] = movie_stats['movieId'].astype(ratings_df['movieId'].dtype)\n",
    "\n",
    "print(\"User Statistics Shape:\", user_stats.shape)\n",
    "print(\"Movie Statistics Shape:\", movie_stats.shape)\n",
    "print(\"\\nData types check:\")\n",
    "print(f\"ratings_df userId dtype: {ratings_df['userId'].dtype}\")\n",
    "print(f\"user_stats userId dtype: {user_stats['userId'].dtype}\")\n",
    "print(f\"ratings_df movieId dtype: {ratings_df['movieId'].dtype}\")\n",
    "print(f\"movie_stats movieId dtype: {movie_stats['movieId'].dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1627488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Merging statistics with ratings data...\n",
      "   - Merging user statistics...\n",
      "   - User stats merged. Shape: (3000000, 7)\n",
      "   - Merging movie statistics...\n",
      "   - Movie stats merged. Shape: (3000000, 10)\n",
      "✅ Step 1: Basic statistics merged successfully!\n",
      "\n",
      "Step 2: Creating advanced features...\n",
      "   - Creating SVD features for collaborative filtering...\n",
      "     (Using efficient sparse matrix approach for large dataset)\n",
      "     - Applying SVD with 80 components...\n",
      "   - SVD features created: 160 features\n",
      "   - Incorporating movies metadata...\n",
      "   - Movies metadata features: 16 features\n",
      "   - Creating enhanced statistical features...\n",
      "     - Calculating min/max ratings...\n",
      "     - Creating simplified percentile features (memory-efficient)...\n",
      "     - Quantile features created successfully\n",
      "     - Creating interaction features...\n",
      "     - Creating density features...\n",
      "     - Creating variance features...\n",
      "     - Creating time-based features...\n",
      "     - Optimizing data types for memory efficiency...\n",
      "     - Memory optimization complete\n",
      "     - Collecting feature names...\n",
      "     - Total features available: 206\n",
      "     - Creating feature matrix...\n",
      "     - Feature matrix created\n",
      "\n",
      "✅ Feature Engineering Complete!\n",
      "Total features: 206\n",
      "  - Base features: 6\n",
      "  - SVD features: 160\n",
      "  - Interaction features: 9\n",
      "  - Statistical features: 10\n",
      "  - Metadata features: 16\n",
      "  - Time features: 5\n",
      "\n",
      "Feature Data Shape: (3000100, 206)\n",
      "Target Data Shape: (3000100,)\n"
     ]
    }
   ],
   "source": [
    "# Merge statistics with ratings data\n",
    "print(\"Step 1: Merging statistics with ratings data...\")\n",
    "\n",
    "# Memory optimization: Sample data if too large (uncomment to use)\n",
    "USE_SAMPLE = False  # Set to True if you encounter memory errors\n",
    "SAMPLE_SIZE = 2000000  # 2M samples (adjust based on available RAM)\n",
    "\n",
    "if USE_SAMPLE and len(ratings_df) > SAMPLE_SIZE:\n",
    "    print(f\"   - Dataset too large ({len(ratings_df):,} rows). Sampling {SAMPLE_SIZE:,} rows for memory efficiency...\")\n",
    "    ratings_df = ratings_df.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "    print(f\"   - Sampled dataset shape: {ratings_df.shape}\")\n",
    "\n",
    "# Ensure data types match before merging\n",
    "ratings_df['userId'] = ratings_df['userId'].astype(user_stats['userId'].dtype)\n",
    "ratings_df['movieId'] = ratings_df['movieId'].astype(movie_stats['movieId'].dtype)\n",
    "\n",
    "# Perform merges with error handling\n",
    "try:\n",
    "    print(\"   - Merging user statistics...\")\n",
    "    df = ratings_df.merge(user_stats, on='userId', how='left', suffixes=('', '_user'))\n",
    "    print(f\"   - User stats merged. Shape: {df.shape}\")\n",
    "    \n",
    "    print(\"   - Merging movie statistics...\")\n",
    "    df = df.merge(movie_stats, on='movieId', how='left', suffixes=('', '_movie'))\n",
    "    print(f\"   - Movie stats merged. Shape: {df.shape}\")\n",
    "    \n",
    "    # Check for duplicate columns and drop if any\n",
    "    duplicate_cols = [col for col in df.columns if df.columns.tolist().count(col) > 1]\n",
    "    if duplicate_cols:\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        print(f\"   - Removed duplicate columns: {duplicate_cols}\")\n",
    "    \n",
    "    print(\"✅ Step 1: Basic statistics merged successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during merge: {e}\")\n",
    "    print(f\"ratings_df columns: {ratings_df.columns.tolist()}\")\n",
    "    print(f\"user_stats columns: {user_stats.columns.tolist()}\")\n",
    "    print(f\"movie_stats columns: {movie_stats.columns.tolist()}\")\n",
    "    raise\n",
    "\n",
    "# Advanced Feature Engineering for Higher Accuracy\n",
    "print(\"\\nStep 2: Creating advanced features...\")\n",
    "\n",
    "# ========== SVD Matrix Factorization Features (Collaborative Filtering) ==========\n",
    "print(\"   - Creating SVD features for collaborative filtering...\")\n",
    "print(\"     (Using efficient sparse matrix approach for large dataset)\")\n",
    "\n",
    "try:\n",
    "    # Sample data for SVD (use larger sample but still manageable)\n",
    "    svd_sample_size = min(50000, len(df))\n",
    "    svd_sample_idx = np.random.choice(len(df), svd_sample_size, replace=False)\n",
    "    df_svd_sample = df.iloc[svd_sample_idx].copy()\n",
    "    \n",
    "    # Create sparse user-item matrix for efficiency\n",
    "    user_ids = df_svd_sample['userId'].unique()\n",
    "    movie_ids = df_svd_sample['movieId'].unique()\n",
    "    \n",
    "    # Create mapping dictionaries\n",
    "    user_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "    movie_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "    \n",
    "    # Build sparse matrix\n",
    "    rows = [user_map[uid] for uid in df_svd_sample['userId']]\n",
    "    cols = [movie_map[mid] for mid in df_svd_sample['movieId']]\n",
    "    values = df_svd_sample['rating'].values\n",
    "    \n",
    "    user_item_sparse = sparse.csr_matrix(\n",
    "        (values, (rows, cols)), \n",
    "        shape=(len(user_ids), len(movie_ids))\n",
    "    )\n",
    "    \n",
    "    # Apply SVD with more components for better accuracy\n",
    "    n_components = 80  # Increased latent factors\n",
    "    print(f\"     - Applying SVD with {n_components} components...\")\n",
    "    \n",
    "    # User factors\n",
    "    svd_user = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    user_factors = svd_user.fit_transform(user_item_sparse)\n",
    "    user_factor_df = pd.DataFrame(\n",
    "        user_factors,\n",
    "        index=user_ids,\n",
    "        columns=[f'user_svd_{i}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # Movie factors (transpose)\n",
    "    svd_movie = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    movie_factors = svd_movie.fit_transform(user_item_sparse.T)\n",
    "    movie_factor_df = pd.DataFrame(\n",
    "        movie_factors,\n",
    "        index=movie_ids,\n",
    "        columns=[f'movie_svd_{i}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # Merge SVD features (fill missing with 0)\n",
    "    df = df.merge(user_factor_df, left_on='userId', right_index=True, how='left')\n",
    "    df = df.merge(movie_factor_df, left_on='movieId', right_index=True, how='left')\n",
    "    \n",
    "    # Fill NaN with mean of each column for better performance\n",
    "    svd_cols = [col for col in df.columns if 'svd' in col]\n",
    "    for col in svd_cols:\n",
    "        df[col] = df[col].fillna(df[col].mean() if df[col].notna().any() else 0)\n",
    "    \n",
    "    print(f\"   - SVD features created: {n_components * 2} features\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   - Warning: SVD failed ({e}), continuing without SVD features...\")\n",
    "    svd_features = []\n",
    "\n",
    "# ========== Movies Metadata Features ==========\n",
    "print(\"   - Incorporating movies metadata...\")\n",
    "try:\n",
    "    # Process movies metadata\n",
    "    movies_df_clean = movies_df.copy()\n",
    "    \n",
    "    # Clean movie ID column\n",
    "    if 'id' in movies_df_clean.columns:\n",
    "        movies_df_clean['id'] = pd.to_numeric(movies_df_clean['id'], errors='coerce')\n",
    "    \n",
    "    # Extract genres\n",
    "    def extract_genres(x):\n",
    "        if pd.isna(x):\n",
    "            return []\n",
    "        try:\n",
    "            if isinstance(x, str):\n",
    "                genres_list = ast.literal_eval(x)\n",
    "                if isinstance(genres_list, list):\n",
    "                    return [g.get('name', '') for g in genres_list if isinstance(g, dict)]\n",
    "            return []\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    movies_df_clean['genres_list'] = movies_df_clean['genres'].apply(extract_genres)\n",
    "    \n",
    "    # Create genre features (top genres)\n",
    "    all_genres = []\n",
    "    for genres in movies_df_clean['genres_list']:\n",
    "        all_genres.extend(genres)\n",
    "    top_genres = pd.Series(all_genres).value_counts().head(10).index.tolist()\n",
    "    \n",
    "    for genre in top_genres:\n",
    "        movies_df_clean[f'has_genre_{genre.replace(\" \", \"_\").lower()}'] = movies_df_clean['genres_list'].apply(\n",
    "            lambda x: 1 if genre in x else 0\n",
    "        )\n",
    "    \n",
    "    # Extract numeric features\n",
    "    numeric_features = ['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']\n",
    "    for feat in numeric_features:\n",
    "        if feat in movies_df_clean.columns:\n",
    "            movies_df_clean[feat] = pd.to_numeric(movies_df_clean[feat], errors='coerce')\n",
    "    \n",
    "    # Merge with main dataframe\n",
    "    if 'id' in movies_df_clean.columns:\n",
    "        df = df.merge(\n",
    "            movies_df_clean[['id'] + [f'has_genre_{g.replace(\" \", \"_\").lower()}' for g in top_genres] + \n",
    "                          [f for f in numeric_features if f in movies_df_clean.columns]],\n",
    "            left_on='movieId',\n",
    "            right_on='id',\n",
    "            how='left'\n",
    "        )\n",
    "        df = df.drop('id', axis=1, errors='ignore')\n",
    "    \n",
    "    metadata_features = [f'has_genre_{g.replace(\" \", \"_\").lower()}' for g in top_genres] + \\\n",
    "                       [f for f in numeric_features if f in movies_df_clean.columns and f in df.columns]\n",
    "    print(f\"   - Movies metadata features: {len(metadata_features)} features\")\n",
    "except Exception as e:\n",
    "    print(f\"   - Warning: Could not load all movies metadata: {e}\")\n",
    "    metadata_features = []\n",
    "\n",
    "# ========== Enhanced Statistical Features ==========\n",
    "print(\"   - Creating enhanced statistical features...\")\n",
    "\n",
    "# Rating statistics by user and movie (more detailed) - using efficient transforms\n",
    "print(\"     - Calculating min/max ratings...\")\n",
    "df['user_rating_max'] = df.groupby('userId')['rating'].transform('max').astype('float32')\n",
    "df['user_rating_min'] = df.groupby('userId')['rating'].transform('min').astype('float32')\n",
    "df['movie_rating_max'] = df.groupby('movieId')['rating'].transform('max').astype('float32')\n",
    "df['movie_rating_min'] = df.groupby('movieId')['rating'].transform('min').astype('float32')\n",
    "\n",
    "# Skip percentile features (too memory-intensive for large datasets)\n",
    "# Use simpler binning instead for memory efficiency\n",
    "print(\"     - Creating simplified percentile features (memory-efficient)...\")\n",
    "try:\n",
    "    # Use simpler quantile-based features that don't require ranking entire groups\n",
    "    user_quantiles = df.groupby('userId')['rating'].quantile([0.25, 0.5, 0.75]).unstack(level=1)\n",
    "    user_quantiles.columns = ['user_q25', 'user_q50', 'user_q75']\n",
    "    df = df.merge(user_quantiles, left_on='userId', right_index=True, how='left')\n",
    "    df[['user_q25', 'user_q50', 'user_q75']] = df[['user_q25', 'user_q50', 'user_q75']].fillna(0).astype('float32')\n",
    "    \n",
    "    movie_quantiles = df.groupby('movieId')['rating'].quantile([0.25, 0.5, 0.75]).unstack(level=1)\n",
    "    movie_quantiles.columns = ['movie_q25', 'movie_q50', 'movie_q75']\n",
    "    df = df.merge(movie_quantiles, left_on='movieId', right_index=True, how='left')\n",
    "    df[['movie_q25', 'movie_q50', 'movie_q75']] = df[['movie_q25', 'movie_q50', 'movie_q75']].fillna(0).astype('float32')\n",
    "    \n",
    "    print(\"     - Quantile features created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"     - Warning: Quantile features failed ({e}), skipping...\")\n",
    "    # Add dummy columns to avoid errors later\n",
    "    df['user_q25'] = df['user_q50'] = df['user_q75'] = 0.0\n",
    "    df['movie_q25'] = df['movie_q50'] = df['movie_q75'] = 0.0\n",
    "\n",
    "# Interaction features (enhanced) - using float32 for memory efficiency\n",
    "print(\"     - Creating interaction features...\")\n",
    "df['user_movie_interaction'] = (df['user_avg_rating'] * df['movie_avg_rating']).astype('float32')\n",
    "df['rating_count_interaction'] = (df['user_rating_count'] * df['movie_rating_count']).astype('float32')\n",
    "df['user_normalized_rating'] = (df['user_avg_rating'] - df['movie_avg_rating']).astype('float32')\n",
    "df['user_movie_ratio'] = (df['user_avg_rating'] / (df['movie_avg_rating'] + 0.01)).astype('float32')\n",
    "df['count_ratio'] = (df['user_rating_count'] / (df['movie_rating_count'] + 1)).astype('float32')\n",
    "\n",
    "# Rating density and normalized features\n",
    "print(\"     - Creating density features...\")\n",
    "max_user_count = df['user_rating_count'].max()\n",
    "max_movie_count = df['movie_rating_count'].max()\n",
    "df['user_rating_density'] = (df['user_rating_count'] / (max_user_count + 1)).astype('float32')\n",
    "df['movie_rating_density'] = (df['movie_rating_count'] / (max_movie_count + 1)).astype('float32')\n",
    "\n",
    "# Variance features\n",
    "print(\"     - Creating variance features...\")\n",
    "df['user_movie_std_product'] = (df['user_rating_std'] * df['movie_rating_std']).astype('float32')\n",
    "df['user_movie_std_diff'] = (abs(df['user_rating_std'] - df['movie_rating_std'])).astype('float32')\n",
    "\n",
    "# Time-based features - using smaller data types\n",
    "print(\"     - Creating time-based features...\")\n",
    "if 'timestamp' in df.columns:\n",
    "    # Convert timestamp in chunks to avoid memory issues\n",
    "    df['timestamp_dt'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')\n",
    "    df['year'] = df['timestamp_dt'].dt.year.astype('int16')\n",
    "    df['month'] = df['timestamp_dt'].dt.month.astype('int8')\n",
    "    df['day_of_week'] = df['timestamp_dt'].dt.dayofweek.astype('int8')\n",
    "    df['hour'] = df['timestamp_dt'].dt.hour.astype('int8')\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype('int8')\n",
    "    df = df.drop('timestamp_dt', axis=1, errors='ignore')\n",
    "    time_features = ['year', 'month', 'day_of_week', 'hour', 'is_weekend']\n",
    "else:\n",
    "    time_features = []\n",
    "\n",
    "# Optimize data types for memory efficiency\n",
    "print(\"     - Optimizing data types for memory efficiency...\")\n",
    "if 'user_rating_count' in df.columns:\n",
    "    df['user_rating_count'] = df['user_rating_count'].astype('int32')\n",
    "if 'movie_rating_count' in df.columns:\n",
    "    df['movie_rating_count'] = df['movie_rating_count'].astype('int32')\n",
    "if 'userId' in df.columns:\n",
    "    df['userId'] = df['userId'].astype('int32')\n",
    "if 'movieId' in df.columns:\n",
    "    df['movieId'] = df['movieId'].astype('int32')\n",
    "\n",
    "# Convert float64 to float32 where possible (saves 50% memory)\n",
    "float_cols = df.select_dtypes(include=['float64']).columns\n",
    "for col in float_cols:\n",
    "    if col != 'rating':  # Keep rating as float64 for precision\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "# Handle missing values\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Replace infinities\n",
    "df = df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "print(\"     - Memory optimization complete\")\n",
    "\n",
    "# Collect all feature names\n",
    "print(\"     - Collecting feature names...\")\n",
    "base_features = ['user_avg_rating', 'user_rating_count', 'user_rating_std',\n",
    "                'movie_avg_rating', 'movie_rating_count', 'movie_rating_std']\n",
    "svd_features = [col for col in df.columns if 'svd' in col]\n",
    "interaction_features = ['user_movie_interaction', 'rating_count_interaction', \n",
    "                       'user_normalized_rating', 'user_movie_ratio', 'count_ratio',\n",
    "                       'user_rating_density', 'movie_rating_density',\n",
    "                       'user_movie_std_product', 'user_movie_std_diff']\n",
    "stat_features = ['user_rating_max', 'user_rating_min', 'movie_rating_max', \n",
    "                'movie_rating_min', 'user_q25', 'user_q50', 'user_q75',\n",
    "                'movie_q25', 'movie_q50', 'movie_q75']\n",
    "\n",
    "features = base_features + svd_features + interaction_features + stat_features + metadata_features + time_features\n",
    "\n",
    "# Remove any features that don't exist\n",
    "features = [f for f in features if f in df.columns]\n",
    "\n",
    "print(f\"     - Total features available: {len(features)}\")\n",
    "\n",
    "# Create feature matrix - use efficient memory allocation\n",
    "print(\"     - Creating feature matrix...\")\n",
    "X = df[features].copy()\n",
    "y = df['rating'].copy()\n",
    "\n",
    "# Force garbage collection to free memory\n",
    "import gc\n",
    "del df  # Delete original dataframe to free memory\n",
    "gc.collect()\n",
    "\n",
    "print(\"     - Feature matrix created\")\n",
    "\n",
    "print(f\"\\n✅ Feature Engineering Complete!\")\n",
    "print(f\"Total features: {len(features)}\")\n",
    "print(f\"  - Base features: {len(base_features)}\")\n",
    "print(f\"  - SVD features: {len(svd_features)}\")\n",
    "print(f\"  - Interaction features: {len(interaction_features)}\")\n",
    "print(f\"  - Statistical features: {len(stat_features)}\")\n",
    "print(f\"  - Metadata features: {len(metadata_features)}\")\n",
    "print(f\"  - Time features: {len(time_features)}\")\n",
    "print(f\"\\nFeature Data Shape: {X.shape}\")\n",
    "print(f\"Target Data Shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c32afe2",
   "metadata": {},
   "source": [
    "## 3. Split Data into Training and Testing Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2452c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2400080\n",
      "Testing set size: 600020\n"
     ]
    }
   ],
   "source": [
    "# Split data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f012c1",
   "metadata": {},
   "source": [
    "## 4. Train Machine Learning Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3a278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression model trained!\n",
      "Basic XGBoost model trained!\n"
     ]
    }
   ],
   "source": [
    "# Train Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"Linear Regression model trained!\")\n",
    "\n",
    "# Train basic XGBoost Model (for comparison)\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1, verbosity=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"Basic XGBoost model trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba115c",
   "metadata": {},
   "source": [
    "## 4b. Hyperparameter Tuning with Random Search CV (XGBoost)\n",
    "\n",
    "We'll use a sample of the data for faster hyperparameter tuning, then train the best model on the full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b07c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1,000,000 samples for hyperparameter tuning\n",
      "Original training set size: 2,400,080\n",
      "Sample represents 41.7% of training data\n"
     ]
    }
   ],
   "source": [
    "# Sample data for faster hyperparameter tuning\n",
    "# Using a substantial sample for better hyperparameter estimation\n",
    "sample_size = min(1000000, len(X_train))  # Increased sample size\n",
    "sample_indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "X_train_sample = X_train.iloc[sample_indices]\n",
    "y_train_sample = y_train.iloc[sample_indices]\n",
    "\n",
    "print(f\"Using {len(X_train_sample):,} samples for hyperparameter tuning\")\n",
    "print(f\"Original training set size: {len(X_train):,}\")\n",
    "print(f\"Sample represents {len(X_train_sample)/len(X_train)*100:.1f}% of training data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d59935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded hyperparameter search space defined:\n",
      "  n_estimators: <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x00000235C8F3E200>\n",
      "  max_depth: <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x00000235C800B3D0>\n",
      "  learning_rate: <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000235D1257850>\n",
      "  subsample: <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000235C8BCDDB0>\n",
      "  colsample_bytree: <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000235D1B3F220>\n",
      "  colsample_bylevel: <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000235D1B39120>\n",
      "  gamma: <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000235C9DBB1F0>\n",
      "  min_child_weight: <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x00000235CE805D50>\n",
      "  reg_alpha: <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000235E257BA00>\n",
      "  reg_lambda: <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000235DA556F20>\n",
      "  scale_pos_weight: <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000235E257BB80>\n"
     ]
    }
   ],
   "source": [
    "# Define expanded hyperparameter search space for XGBoost (more aggressive tuning)\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': randint(200, 1000),  # Increased range\n",
    "    'max_depth': randint(5, 15),  # Deeper trees\n",
    "    'learning_rate': uniform(0.005, 0.2),  # Lower learning rates\n",
    "    'subsample': uniform(0.7, 0.3),  # 0.7 to 1.0\n",
    "    'colsample_bytree': uniform(0.7, 0.3),  # 0.7 to 1.0\n",
    "    'colsample_bylevel': uniform(0.7, 0.3),  # Additional sampling\n",
    "    'gamma': uniform(0, 10),\n",
    "    'min_child_weight': randint(1, 15),\n",
    "    'reg_alpha': uniform(0, 15),  # L1 regularization\n",
    "    'reg_lambda': uniform(0, 15),  # L2 regularization\n",
    "    'scale_pos_weight': uniform(0.8, 0.4)\n",
    "}\n",
    "\n",
    "print(\"Expanded hyperparameter search space defined:\")\n",
    "for key, value in xgb_param_grid.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22a71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Enhanced Random Search CV for XGBoost hyperparameter tuning...\n",
      "Using 1,000,000 samples for faster tuning\n",
      "Testing 100 parameter combinations with 3-fold CV...\n",
      "This may take 30-60 minutes...\n",
      "\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "\n",
      "============================================================\n",
      "Random Search CV Completed!\n",
      "============================================================\n",
      "\n",
      "Best parameters found:\n",
      "  colsample_bylevel: 0.726360433372797\n",
      "  colsample_bytree: 0.7416474320326061\n",
      "  gamma: 0.027108994143296705\n",
      "  learning_rate: 0.028339212828336267\n",
      "  max_depth: 12\n",
      "  min_child_weight: 6\n",
      "  n_estimators: 618\n",
      "  reg_alpha: 5.23302399814493\n",
      "  reg_lambda: 13.942937163717387\n",
      "  scale_pos_weight: 1.1322477631150918\n",
      "  subsample: 0.9895080731999537\n",
      "\n",
      "Best cross-validation score (neg MSE): -0.7020\n",
      "Best cross-validation RMSE: 0.8379\n",
      "Best cross-validation R² (estimated): 0.3689\n"
     ]
    }
   ],
   "source": [
    "# Create base XGBoost model with optimized settings\n",
    "xgb_base = xgb.XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0,\n",
    "    tree_method='hist',  # Faster training\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "# Perform Random Search CV with more iterations\n",
    "print(\"Starting Enhanced Random Search CV for XGBoost hyperparameter tuning...\")\n",
    "print(f\"Using {len(X_train_sample):,} samples for faster tuning\")\n",
    "print(f\"Testing {100} parameter combinations with 3-fold CV...\")\n",
    "print(\"This may take 30-60 minutes...\\n\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=xgb_param_grid,\n",
    "    n_iter=100,  # Increased iterations for better search\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # Minimize MSE (maximize neg MSE)\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2,  # More verbose output\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit on sample data\n",
    "random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Random Search CV Completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest parameters found:\")\n",
    "for param, value in sorted(random_search.best_params_.items()):\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest cross-validation score (neg MSE): {random_search.best_score_:.4f}\")\n",
    "print(f\"Best cross-validation RMSE: {np.sqrt(-random_search.best_score_):.4f}\")\n",
    "print(f\"Best cross-validation R² (estimated): {1 - abs(random_search.best_score_) / np.var(y_train_sample):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33498684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training optimized XGBoost model on full training dataset...\n",
      "============================================================\n",
      "Training on 2,400,080 samples...\n",
      "This may take 20-40 minutes...\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m y_val_sample \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Train on full dataset with early stopping\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mxgb_optimized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_sample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[0;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Optimized XGBoost model trained on full dataset!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest iteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxgb_optimized\u001b[38;5;241m.\u001b[39mbest_iteration\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mhasattr\u001b[39m(xgb_optimized,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_iteration\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\vallhalla\\lib\\site-packages\\xgboost\\core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# Train the optimized XGBoost model on full training data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training optimized XGBoost model on full training dataset...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training on {len(X_train):,} samples...\")\n",
    "print(\"This may take 20-40 minutes...\\n\")\n",
    "\n",
    "xgb_optimized = random_search.best_estimator_.__class__(**random_search.best_params_)\n",
    "xgb_optimized.set_params(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0,\n",
    "    tree_method='hist',\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "# Increase n_estimators for full training if it's too low\n",
    "if random_search.best_params_.get('n_estimators', 200) < 300:\n",
    "    xgb_optimized.set_params(n_estimators=500)  # Use more trees for full dataset\n",
    "\n",
    "# Sample for early stopping validation\n",
    "val_size = min(500000, len(X_train) // 5)\n",
    "val_idx = np.random.choice(len(X_train), val_size, replace=False)\n",
    "X_val_sample = X_train.iloc[val_idx]\n",
    "y_val_sample = y_train.iloc[val_idx]\n",
    "\n",
    "# Train on full dataset with early stopping\n",
    "xgb_optimized.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    eval_set=[(X_val_sample, y_val_sample)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Optimized XGBoost model trained on full dataset!\")\n",
    "print(f\"Best iteration: {xgb_optimized.best_iteration if hasattr(xgb_optimized, 'best_iteration') else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f1893",
   "metadata": {},
   "source": [
    "## 5. Make Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f4e66c",
   "metadata": {},
   "source": [
    "## 4c. Ensemble Model (Stacking for Maximum Accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble model by stacking XGBoost with other models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Ensemble Model (XGBoost + Gradient Boosting)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train Gradient Boosting as second model\n",
    "print(\"Training Gradient Boosting Regressor...\")\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train on sample for faster training (use same sample as hyperparameter tuning)\n",
    "gb_model.fit(X_train_sample, y_train_sample)\n",
    "print(\"✅ Gradient Boosting trained!\")\n",
    "\n",
    "# Create ensemble predictions (weighted average)\n",
    "# XGBoost typically performs better, so give it more weight\n",
    "print(\"\\nCreating ensemble predictions...\")\n",
    "xgb_pred_train = xgb_optimized.predict(X_train_sample)\n",
    "gb_pred_train = gb_model.predict(X_train_sample)\n",
    "\n",
    "# Find optimal weights using validation\n",
    "from sklearn.linear_model import Ridge\n",
    "ensemble_train = np.column_stack([xgb_pred_train, gb_pred_train])\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "meta_model.fit(ensemble_train, y_train_sample)\n",
    "\n",
    "print(f\"Ensemble weights - XGBoost: {meta_model.coef_[0]:.3f}, GB: {meta_model.coef_[1]:.3f}\")\n",
    "print(f\"Ensemble intercept: {meta_model.intercept_:.3f}\")\n",
    "\n",
    "# Create ensemble model wrapper\n",
    "class EnsembleModel:\n",
    "    def __init__(self, xgb_model, gb_model, meta_model):\n",
    "        self.xgb_model = xgb_model\n",
    "        self.gb_model = gb_model\n",
    "        self.meta_model = meta_model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        xgb_pred = self.xgb_model.predict(X)\n",
    "        gb_pred = self.gb_model.predict(X)\n",
    "        ensemble_features = np.column_stack([xgb_pred, gb_pred])\n",
    "        return self.meta_model.predict(ensemble_features)\n",
    "\n",
    "ensemble_model = EnsembleModel(xgb_optimized, gb_model, meta_model)\n",
    "print(\"✅ Ensemble model created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0743d",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation - Calculate Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "11f75672",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [600020, 5204858]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 25\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m: mae,\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m: mse,\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m: rmse,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR²\u001b[39m\u001b[38;5;124m'\u001b[39m: r2\n\u001b[0;32m     22\u001b[0m     }\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Evaluate all models\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m lr_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLinear Regression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m xgb_basic_metrics \u001b[38;5;241m=\u001b[39m evaluate_model(y_test, xgb_basic_predictions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost (Basic)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m xgb_optimized_metrics \u001b[38;5;241m=\u001b[39m evaluate_model(y_test, xgb_optimized_predictions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost (Optimized with Random Search CV)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[86], line 3\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(y_true, y_pred, model_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_model\u001b[39m(y_true, y_pred, model_name):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate and display evaluation metrics\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     mae \u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_true, y_pred)\n\u001b[0;32m      5\u001b[0m     rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mse)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\vallhalla\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\vallhalla\\lib\\site-packages\\sklearn\\metrics\\_regression.py:284\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03mThe mean absolute error is a non-negative floating point value, where best value\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m0.85...\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[0;32m    283\u001b[0m _, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 284\u001b[0m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m )\n\u001b[0;32m    289\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m _average(\n\u001b[0;32m    290\u001b[0m     xp\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[0;32m    291\u001b[0m )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\vallhalla\\lib\\site-packages\\sklearn\\metrics\\_regression.py:209\u001b[0m, in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ensures y_true, y_pred, and sample_weight correspond to same regression task.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03mExtends `_check_reg_targets` by automatically selecting a suitable floating-point\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m dtype_name \u001b[38;5;241m=\u001b[39m _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m--> 209\u001b[0m y_type, y_true, y_pred, sample_weight, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_type, y_true, y_pred, sample_weight, multioutput\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\vallhalla\\lib\\site-packages\\sklearn\\metrics\\_regression.py:114\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true, y_pred and sample_weight belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03mTo reduce redundancy when calling `_find_matching_floating_dtype`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    112\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m--> 114\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    116\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\vallhalla\\lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    471\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    474\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    476\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [600020, 5204858]"
     ]
    }
   ],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate and display evaluation metrics\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluation Metrics for {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Mean Absolute Error (MAE):     {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE):      {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"R² Score (R-squared):          {r2:.4f}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "lr_metrics = evaluate_model(y_test, lr_predictions, \"Linear Regression\")\n",
    "xgb_basic_metrics = evaluate_model(y_test, xgb_basic_predictions, \"XGBoost (Basic)\")\n",
    "xgb_optimized_metrics = evaluate_model(y_test, xgb_optimized_predictions, \"XGBoost (Optimized with Random Search CV)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07c4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "      Linear Regression  xgb regressor\n",
      "MAE              0.6678         0.6567\n",
      "MSE              0.7591         0.7421\n",
      "RMSE             0.8713         0.8614\n",
      "R²               0.3313         0.3463\n"
     ]
    }
   ],
   "source": [
    "# Create a comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Linear Regression': [lr_metrics['MAE'], lr_metrics['MSE'], \n",
    "                          lr_metrics['RMSE'], lr_metrics['R²']],\n",
    "    'XGBoost (Basic)': [xgb_basic_metrics['MAE'], xgb_basic_metrics['MSE'], \n",
    "                        xgb_basic_metrics['RMSE'], xgb_basic_metrics['R²']],\n",
    "    'XGBoost (Optimized)': [xgb_optimized_metrics['MAE'], xgb_optimized_metrics['MSE'], \n",
    "                            xgb_optimized_metrics['RMSE'], xgb_optimized_metrics['R²']]\n",
    "}, index=['MAE', 'MSE', 'RMSE', 'R²'])\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = {\n",
    "    'MAE': ((xgb_basic_metrics['MAE'] - xgb_optimized_metrics['MAE']) / xgb_basic_metrics['MAE'] * 100),\n",
    "    'MSE': ((xgb_basic_metrics['MSE'] - xgb_optimized_metrics['MSE']) / xgb_basic_metrics['MSE'] * 100),\n",
    "    'RMSE': ((xgb_basic_metrics['RMSE'] - xgb_optimized_metrics['RMSE']) / xgb_basic_metrics['RMSE'] * 100),\n",
    "    'R²': ((xgb_optimized_metrics['R²'] - xgb_basic_metrics['R²']) / abs(xgb_basic_metrics['R²']) * 100)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Improvement from Basic to Optimized XGBoost:\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in improvement.items():\n",
    "    print(f\"{metric}: {value:.2f}% improvement\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba6ac9",
   "metadata": {},
   "source": [
    "## 7. Visualization of Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f68f34",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['MAE', 'MSE', 'RMSE'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Metrics comparison bar plot\u001b[39;00m\n\u001b[0;32m      5\u001b[0m metrics_to_plot \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m \u001b[43mcomparison_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetrics_to_plot\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m, ax\u001b[38;5;241m=\u001b[39maxes[\u001b[38;5;241m0\u001b[39m], rot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Comparison: MAE, MSE, RMSE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError Value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\vallhalla\\lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4118\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4119\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4121\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\vallhalla\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\vallhalla\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6261\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6263\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['MAE', 'MSE', 'RMSE'], dtype='object')] are in the [columns]\""
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot 1: Comparison of metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Metrics comparison bar plot\n",
    "metrics_to_plot = ['MAE', 'MSE', 'RMSE']\n",
    "comparison_df[metrics_to_plot].T.plot(kind='bar', ax=axes[0], rot=0, width=0.8)\n",
    "axes[0].set_title('Model Comparison: MAE, MSE, RMSE (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Error Value')\n",
    "axes[0].legend(title='Model', loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_xticklabels(metrics_to_plot)\n",
    "\n",
    "# R² comparison\n",
    "r2_comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'XGBoost (Basic)', 'XGBoost (Optimized)'],\n",
    "    'R² Score': [lr_metrics['R²'], xgb_basic_metrics['R²'], xgb_optimized_metrics['R²']]\n",
    "})\n",
    "r2_comparison.plot(x='Model', y='R² Score', kind='bar', ax=axes[1], \n",
    "                   color=['skyblue', 'lightcoral', 'lightgreen'], rot=45, width=0.7)\n",
    "axes[1].set_title('R² Score Comparison (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('R² Score')\n",
    "axes[1].legend().remove()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Actual vs Predicted\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Sample for visualization (too many points)\n",
    "sample_size_viz = min(5000, len(y_test))\n",
    "sample_idx = np.random.choice(len(y_test), sample_size_viz, replace=False)\n",
    "\n",
    "# Optimized XGBoost predictions\n",
    "axes[0].scatter(y_test.iloc[sample_idx], xgb_optimized_predictions[sample_idx], alpha=0.5, s=1)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Ratings')\n",
    "axes[0].set_ylabel('Predicted Ratings')\n",
    "axes[0].set_title('XGBoost (Optimized): Actual vs Predicted')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Basic XGBoost predictions\n",
    "axes[1].scatter(y_test.iloc[sample_idx], xgb_basic_predictions[sample_idx], alpha=0.5, s=1)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Ratings')\n",
    "axes[1].set_ylabel('Predicted Ratings')\n",
    "axes[1].set_title('XGBoost (Basic): Actual vs Predicted')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Linear Regression predictions\n",
    "axes[2].scatter(y_test.iloc[sample_idx], lr_predictions[sample_idx], alpha=0.5, s=1)\n",
    "axes[2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[2].set_xlabel('Actual Ratings')\n",
    "axes[2].set_ylabel('Predicted Ratings')\n",
    "axes[2].set_title('Linear Regression: Actual vs Predicted')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Residual plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sample_idx = np.random.choice(len(y_test), min(5000, len(y_test)), replace=False)\n",
    "\n",
    "# Optimized XGBoost residuals\n",
    "xgb_opt_residuals = y_test.iloc[sample_idx] - xgb_optimized_predictions[sample_idx]\n",
    "axes[0].scatter(xgb_optimized_predictions[sample_idx], xgb_opt_residuals, alpha=0.5, s=1)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[0].set_xlabel('Predicted Ratings')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('XGBoost (Optimized): Residual Plot')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Basic XGBoost residuals\n",
    "xgb_basic_residuals = y_test.iloc[sample_idx] - xgb_basic_predictions[sample_idx]\n",
    "axes[1].scatter(xgb_basic_predictions[sample_idx], xgb_basic_residuals, alpha=0.5, s=1)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Ratings')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('XGBoost (Basic): Residual Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Linear Regression residuals\n",
    "lr_residuals = y_test.iloc[sample_idx] - lr_predictions[sample_idx]\n",
    "axes[2].scatter(lr_predictions[sample_idx], lr_residuals, alpha=0.5, s=1)\n",
    "axes[2].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[2].set_xlabel('Predicted Ratings')\n",
    "axes[2].set_ylabel('Residuals')\n",
    "axes[2].set_title('Linear Regression: Residual Plot')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809202d",
   "metadata": {},
   "source": [
    "## 8. Feature Importance (XGBoost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importance from Optimized XGBoost\n",
    "feature_importance_opt = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': xgb_optimized.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "feature_importance_basic = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance - XGBoost (Optimized):\")\n",
    "print(feature_importance_opt)\n",
    "\n",
    "print(\"\\nFeature Importance - XGBoost (Basic):\")\n",
    "print(feature_importance_basic)\n",
    "\n",
    "# Visualize feature importance comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Optimized model\n",
    "axes[0].barh(feature_importance_opt['Feature'], feature_importance_opt['Importance'], color='lightgreen')\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Feature Importance - XGBoost (Optimized)', fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Basic model\n",
    "axes[1].barh(feature_importance_basic['Feature'], feature_importance_basic['Importance'], color='lightcoral')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Feature Importance - XGBoost (Basic)', fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16ddbe",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This project successfully:\n",
    "- ✅ Loaded and explored the movie ratings dataset\n",
    "- ✅ Engineered features from user and movie statistics\n",
    "- ✅ Trained two machine learning models (Linear Regression and Random Forest)\n",
    "- ✅ Evaluated models using:\n",
    "  - **MAE** (Mean Absolute Error)\n",
    "  - **MSE** (Mean Squared Error)\n",
    "  - **RMSE** (Root Mean Squared Error)\n",
    "  - **R²** (R-squared/coefficient of determination)\n",
    "- ✅ Created visualizations to understand model performance\n",
    "\n",
    "The Random Forest model typically performs better than Linear Regression for this type of problem, capturing non-linear relationships in the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vallhalla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
